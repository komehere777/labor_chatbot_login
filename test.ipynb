{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymongo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_all_contents\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OPENAI_API_KEY\n",
      "File \u001b[1;32md:\\labor_chatbot\\model.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MongoClient\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MONGO_URI, MONGO_DBNAME\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_mongo_client\u001b[39m():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymongo'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from model import get_all_contents\n",
    "import os\n",
    "from config import OPENAI_API_KEY\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "VECTORSTORE_PATH = \"vectorstore.faiss\"\n",
    "\n",
    "\n",
    "def initialize_vectorstore():\n",
    "    sections = get_all_contents()\n",
    "\n",
    "    # RecursiveCharacterTextSplitter 사용\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "    # 섹션을 분할하여 문서 리스트 생성\n",
    "    documents = []\n",
    "    for section in sections:\n",
    "        splits = text_splitter.split_text(section)\n",
    "        documents.extend([Document(page_content=split) for split in splits])\n",
    "\n",
    "    # 문서에 대한 임베딩 생성 및 벡터스토어 초기화\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "    # 벡터스토어를 로컬에 저장\n",
    "    vectorstore.save_local(VECTORSTORE_PATH)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def get_relevant_sections(user_input):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.load_local(\n",
    "        VECTORSTORE_PATH, embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "\n",
    "    # 가장 유사한 벡터 검색\n",
    "    # retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    # 가장 유사한 결과를 찾으면서 결과 간의 중복을 최소화. 유사성과 다양성을 동일하게 고려\n",
    "    retriever = vectorstore.as_retriever(search_type=\"mmr\", k=5, lambda_mult=0.5)\n",
    "\n",
    "    relevant_documents = retriever.invoke(user_input)\n",
    "    relevant_sections = \"\\n\".join([doc.page_content for doc in relevant_documents])\n",
    "\n",
    "    return relevant_sections\n",
    "\n",
    "\n",
    "def get_ai_response(user_input, chat_history):\n",
    "\n",
    "    template = \"\"\"\n",
    "        ## 당신은 근로자들을 상담해주는 근로자 상담 관련 챗봇입니다.\n",
    "        - 법률에 관한 db는 사용자가 관련된 법이나 법 조항을 물어보고 꼭 필요할때 사용하세요.\n",
    "        - 당신의 역할은 근로기준법을 모르는 사람에게 관련 법과 이에 관한 제도들을 설명해주는 것입니다.\n",
    "        - 답변이 정확하지 않을 가능성이 높을 경우 잘 모른다고 답변해야 됩니다. 부정확한 답변은 하지 않는게 좋습니다.\n",
    "        - 질문이 구체적이라면 그 방법도 구체적으로 알려주는것이 좋습니다.\n",
    "        - 유저의 질문에 정보가 많이 없을때는 한번 더 질문을 해서 더 구체적인 답변을 주는것이 좋습니다. 예를 들면 월급이나 수당관련되서는 날짜를 다시 물어본 후 계산을 해줄지 한번 더 물어본다음 계산을 해주면 원하는 답변을 해줄 수 있어요.\n",
    "        - 한 가지 주제에 대한 질문에 답변 후, 사용자가 놓쳤을 수 있는 관련 주제에 대해 추가 질문을 제안해 주세요.\n",
    "        - 답변에 대해서는 최대한 상세하게 알려주세요.\n",
    "        - 금액 산정에 관한 질문에는 돈의 성격에 따라서 세금이나, 수당, 공제 등 여러가지 변수들이 있을 수 있음으로 성격에 맞게 이를 명시하여 주세요.\n",
    "        - 법률적 조언과 일반적인 정보 제공의 차이를 명확히 해주세요. 법률에 관한 정보를 줄 수는 있으나, 전문 변호사나 상담 기관에 문의할 것을 권장해 주세요.\n",
    "\n",
    "        1. \n",
    "\n",
    "        문서 내용: {context}\n",
    "\n",
    "        대화 내용:\n",
    "        {chat_history}\n",
    "\n",
    "        질문: {human_input}\n",
    "\n",
    "        응답:\n",
    "    \"\"\"\n",
    "    # 템플릿을 사용하여 프롬프트 생성\n",
    "    custom_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # 컨텍스트를 가져오는 부분 (사용자 정의 함수라고 가정)\n",
    "    context = get_relevant_sections(user_input)\n",
    "\n",
    "    # LLM 생성\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini-2024-07-18\", temperature=0.2)\n",
    "\n",
    "    # 체인 생성\n",
    "    chain = custom_prompt | llm\n",
    "\n",
    "    # 기존 대화 내역 포함\n",
    "    inputs = {\n",
    "        \"context\": context,\n",
    "        \"human_input\": user_input,\n",
    "        \"chat_history\": chat_history,\n",
    "    }\n",
    "    response = chain.invoke(inputs)\n",
    "\n",
    "    # 대화 내역 저장\n",
    "    chat_history = chat_history + f\"\\nUser: {user_input}\\nAI: {response.content}\"\n",
    "\n",
    "    return response.content, chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
